{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-7YpO3eThBL",
        "outputId": "46928746-d903-41bb-e3c5-0891d87b221f"
      },
      "outputs": [],
      "source": [
        "!pip install semantichar==0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3Nq2uZgTq8W",
        "outputId": "6f2e5788-521d-4f92-d937-47821fc8797f"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/xiyuanzh/SHARE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4uwW2Q_UllV",
        "outputId": "7f36e386-ca35-4d80-e5d3-eb7358650d8b"
      },
      "outputs": [],
      "source": [
        "cd ./SHARE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNPs-Tj7v8a2"
      },
      "outputs": [],
      "source": [
        "mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG1ZA18YUnuC"
      },
      "outputs": [],
      "source": [
        "mkdir dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yptu1gkUpUJ",
        "outputId": "30157529-c5e6-4dee-b27b-7ead2743b1dc"
      },
      "outputs": [],
      "source": [
        "cd ./dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfM7CVWjUq9e",
        "outputId": "ee11b709-c0e3-406f-cc41-f78debdb5ad7"
      },
      "outputs": [],
      "source": [
        "!unzip ./easy_imu_phone.zip -d ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fRitk23U-y1",
        "outputId": "d893396c-9fbf-4f1b-9ac0-bf5566e7b282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/keerthiv/HAR_models/SHARE/SHARE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/keerthiv/deep_conv_lstm/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "cd /home/keerthiv/HAR_models/SHARE/SHARE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from semantichar.utils import all_label_augmentation\n",
        "\n",
        "def DataBatch(data, label, text, l, batchsize, shuffle=True):\n",
        "    \n",
        "    n = data.shape[0]\n",
        "    if shuffle:\n",
        "        index = np.random.permutation(n)\n",
        "    else:\n",
        "        index = np.arange(n)\n",
        "    for i in range(int(np.ceil(n/batchsize))):\n",
        "        inds = index[i*batchsize : min(n,(i+1)*batchsize)]\n",
        "        yield data[inds], label[inds], text[inds], l[inds]\n",
        "        \n",
        "def trainer(config, \n",
        "            enc, \n",
        "            dec, \n",
        "            cross_entropy, \n",
        "            optimizer, \n",
        "            tr_data, \n",
        "            tr_label, \n",
        "            tr_text, \n",
        "            len_text, \n",
        "            break_step, \n",
        "            vocab_size, \n",
        "            device\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    Args:\n",
        "        config: user-specified configurations.\n",
        "        qat_model: encoder of the model.\n",
        "        dec: decoder of the model.\n",
        "        cross_entropy: loss function.\n",
        "        optimizer: optimizer (default is Adam).\n",
        "        tr_data, tr_label, tr_text, len_text: training data, label, label sequence, length of the label sequence. \n",
        "        break_step: length of the longest label sequence length (i.e., maximum decoding step).\n",
        "        vocab_size: label name vocabulary size.\n",
        "        device: cuda or cpu.\n",
        "    \"\"\"\n",
        "\n",
        "    enc.train()\n",
        "    dec.train()  \n",
        "    scaler = GradScaler()\n",
        "    total_loss = 0\n",
        "    for batch_data, batch_label, batch_text, batch_len in \\\n",
        "        DataBatch(tr_data, tr_label, tr_text, len_text, config['batchSize']):\n",
        "        \n",
        "        batch_text = all_label_augmentation(batch_text, config['prob'], break_step, vocab_size)\n",
        "\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_label = batch_label.to(device)\n",
        "        batch_text = batch_text.to(device)\n",
        "        batch_len = batch_len.to(device)\n",
        "\n",
        "        with autocast():\n",
        "            enc_hidden = enc(batch_data)\n",
        "        \n",
        "            enc_hidden = enc(batch_data)\n",
        "            pred, batch_text_sorted, decode_lengths, sort_ind \\\n",
        "                = dec(enc_hidden, batch_text, batch_len)\n",
        "            \n",
        "            targets = batch_text_sorted[:, 1:]\n",
        "\n",
        "            pred, *_ = pack_padded_sequence(pred, decode_lengths, batch_first=True)\n",
        "            targets, *_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            loss = cross_entropy(pred, targets.long())\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += len(batch_data) * loss.item()\n",
        "\n",
        "    total_loss /= len(tr_data)\n",
        "    \n",
        "    return total_loss \n",
        "\n",
        "  \n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "def evaluate(opt, \n",
        "             enc, \n",
        "             dec, \n",
        "             test_data, \n",
        "             test_label, \n",
        "             test_text, \n",
        "             test_len_text, \n",
        "             pred_dict, \n",
        "             seqs, \n",
        "             break_step, \n",
        "             class_num, \n",
        "             vocab_size, \n",
        "             device,\n",
        "             load=True):\n",
        "    \"\"\"\n",
        "    Evaluate the model.\n",
        "    Args:\n",
        "        opt: user-specified configurations.\n",
        "        enc: encoder of the model.\n",
        "        dec: decoder of the model.\n",
        "        test_data, test_label, test_text, test_len_text: test data, label, label sequence, length of the label sequence.\n",
        "        pred_dict: mapping from label token-id sequence to label id.\n",
        "        seqs: label token-id sequence for all classes.\n",
        "        break_step: length of the longest label sequence length (i.e., maximum decoding step).\n",
        "        class_num: number of classes.\n",
        "        vocab_size: label name vocabulary size.\n",
        "        device: cuda or cpu.\n",
        "        load: load saved model weights or not.\n",
        "    \"\"\"\n",
        "\n",
        "    #enc.eval()\n",
        "    #dec.eval()\n",
        "    enc.cpu()\n",
        "    dec.cpu()\n",
        "    enc.eval()\n",
        "    quantized_enc = torch.quantization.convert(enc, inplace=False)\n",
        "    print(\"enc Model converted to quantized version\")\n",
        "\n",
        "\n",
        "    dec.eval()\n",
        "    quantized_dec = torch.quantization.convert(dec, inplace=False)\n",
        "    print(\"dec Model converted to quantized version\")\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    quantized_enc.eval()\n",
        "    quantized_dec.eval()\n",
        "    quantized_enc.to(device)\n",
        "    quantized_dec.to(device)\n",
        "    #if load:\n",
        "    #    enc.load_state_dict(torch.load(opt['model_path'] + opt['run_tag'] + '_enc.pth', map_location=device, weights_only=True))\n",
        "    #    dec.load_state_dict(torch.load(opt['model_path'] + opt['run_tag'] + '_dec.pth', map_location=device, weights_only=True))\n",
        "\n",
        "    hypotheses = list()\n",
        "    batch_size = test_data.size(0)\n",
        "    pred_whole = torch.zeros_like(test_label)\n",
        "    seqs = seqs.to(device)\n",
        "\n",
        "    total_evaluation_time = 0  # Initialize total evaluation time\n",
        "    total_samples = 0  # Initialize total number of samples\n",
        "\n",
        "    for batch_idx, (batch_data, batch_label, batch_text, batch_len) in enumerate(\n",
        "        DataBatch(test_data, test_label, test_text, test_len_text, opt['batchSize'], shuffle=False)\n",
        "    ):\n",
        "\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_label = batch_label.to(device)\n",
        "        batch_text = batch_text.to(device)\n",
        "        batch_len = batch_len.to(device)\n",
        "        \n",
        "        # Start timing after sending to device\n",
        "        start_time = time.time()\n",
        "\n",
        "        batch_size = batch_data.size(0)\n",
        "        total_samples += batch_size  # Accumulate the number of samples\n",
        "        encoder_out = quantized_enc(batch_data)  # (batch_size, enc_seq_len, encoder_dim)\n",
        "        enc_seq_len = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(2)\n",
        "\n",
        "        encoder_out = encoder_out.unsqueeze(1).expand(batch_size, class_num, enc_seq_len, encoder_dim)\n",
        "        encoder_out = encoder_out.reshape(batch_size * class_num, enc_seq_len, encoder_dim)\n",
        "\n",
        "        k_prev_words = seqs[:, 0].unsqueeze(0).expand(batch_size, class_num).long()  # (batch_size, class_num)\n",
        "        k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
        "\n",
        "        h, c = quantized_dec.init_hidden_state(encoder_out)\n",
        "\n",
        "        seq_scores = torch.zeros((batch_size, class_num)).to(device)\n",
        "\n",
        "        for step in range(1, break_step):\n",
        "            embeddings = quantized_dec.embedding(k_prev_words).squeeze(1)  # (batch_size * class_num, embed_dim)\n",
        "            h, c = quantized_dec.decode_step(embeddings, (h, c))\n",
        "            scores = quantized_dec.fc(h.reshape(batch_size, class_num, -1))  # (batch_size, class_num, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=-1)\n",
        "            k_prev_words = seqs[:, step].unsqueeze(0).expand(batch_size, class_num).long()\n",
        "            for batch_i in range(batch_size):\n",
        "                for class_i in range(class_num):\n",
        "                    if k_prev_words[batch_i, class_i] != 0:\n",
        "                        seq_scores[batch_i, class_i] += scores[batch_i, class_i, k_prev_words[batch_i, class_i]]\n",
        "            k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
        "\n",
        "        max_indices = seq_scores.argmax(dim=1)\n",
        "        for batch_i in range(batch_size):\n",
        "            max_i = max_indices[batch_i]\n",
        "            seq = seqs[max_i].tolist()\n",
        "            hypotheses.append([w for w in seq if w not in {0, vocab_size - 1}])\n",
        "            pred_whole[batch_i + batch_idx * opt['batchSize']] = pred_dict[\"#\".join(map(str, hypotheses[-1]))]\n",
        "\n",
        "        # End timing for the batch\n",
        "        end_time = time.time()\n",
        "        batch_evaluation_time = end_time - start_time  # Calculate batch evaluation time\n",
        "        total_evaluation_time += batch_evaluation_time  # Accumulate total evaluation time\n",
        "\n",
        "        print(f'Batch {batch_idx + 1} Evaluation Time: {batch_evaluation_time:.2f} seconds')\n",
        "\n",
        "    acc = accuracy_score(test_label.cpu().numpy(), pred_whole.cpu().numpy())\n",
        "    prec = precision_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "    rec = recall_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "    f1 = f1_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "\n",
        "    print(f'Total Evaluation Time: {total_evaluation_time:.2f} seconds')\n",
        "    \n",
        "    # Calculate evaluation time per batch and per sample\n",
        "    eval_time_per_batch = total_evaluation_time / (batch_idx + 1)\n",
        "    eval_time_per_sample = total_evaluation_time / total_samples\n",
        "\n",
        "    print(f'Average Evaluation Time per Batch: {eval_time_per_batch:.2f} seconds')\n",
        "    print(f'Average Evaluation Time per Sample: {eval_time_per_sample:.6f} seconds')\n",
        "\n",
        "    return acc, prec, rec, f1, total_evaluation_time, eval_time_per_batch, eval_time_per_sample\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.quantization\n",
        "\n",
        "import semantichar.data \n",
        "from semantichar import imagebind_model\n",
        "from semantichar.imagebind_model import ModalityType\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torch.quantization as quant\n",
        "class CustomBatchNorm1d(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(CustomBatchNorm1d, self).__init__()\n",
        "        self.bn = nn.BatchNorm1d(num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert batch normalization to quantization-aware operations\n",
        "        if self.training:\n",
        "            x = self.bn(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.batch_norm(\n",
        "                x, \n",
        "                self.bn.running_mean, \n",
        "                self.bn.running_var, \n",
        "                self.bn.weight, \n",
        "                self.bn.bias, \n",
        "                training=False\n",
        "            )\n",
        "        return x\n",
        "    \n",
        "\n",
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, input_size, output_size, bias=True):\n",
        "        super(QuantizedLinear, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size, bias=bias)\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)  # Quantize input\n",
        "        x = self.fc(x)     # Apply linear transformation\n",
        "        x = self.dequant(x)  # Dequantize output\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        # Fuse the linear layer with the quantization stubs\n",
        "        torch.quantization.fuse_modules(self, ['quant', 'fc', 'dequant'], inplace=True)\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self.fc.weight\n",
        "\n",
        "    @property\n",
        "    def bias(self):\n",
        "        return self.fc.bias\n",
        " \n",
        "    \n",
        "class CustomObserver(quant.MinMaxObserver):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomObserver, self).__init__(*args, **kwargs)\n",
        "\n",
        "class QuantizedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
        "        super(QuantizedConv1d, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.bn = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.weight_fake_quant = quant.FakeQuantize.with_args(observer=quant.default_weight_observer, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine)()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)  # Quantize input\n",
        "        x = self.conv(x)   # Apply convolution\n",
        "        x = self.bn(x)     # Apply batch normalization\n",
        "        x = self.relu(x)   # Apply ReLU\n",
        "        x = self.dequant(x)  # Dequantize output\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        # Fuse the conv, batch norm, and ReLU layers\n",
        "        torch.quantization.fuse_modules(self, [['conv', 'bn', 'relu']], inplace=True)\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self.conv.weight\n",
        "\n",
        "    @property\n",
        "    def bias(self):\n",
        "        return self.conv.bias\n",
        "\n",
        "    def apply_weight_fake_quant(self):\n",
        "        self.weight_fake_quant(self.conv.weight)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_input: int, d_model: int, d_output: int, seq_len:int):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv1d( d_input, 128, kernel_size=3, padding=(1))\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=(1))\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        #self.conv3 = nn.Conv1d( 256, 256, kernel_size=3, padding=(1))\n",
        "        #self.bn3 = nn.BatchNorm1d(256)\n",
        "        #self.relu3 = nn.ReLU()\n",
        "        #self.conv4 = nn.Conv1d(256, d_output, kernel_size=3, padding=(1))\n",
        "        #self.bn4 = nn.BatchNorm1d(d_output)\n",
        "        #self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.quant1 = quant.QuantStub()  # Quantizes the input\n",
        "        self.dequant1 = quant.DeQuantStub()  # Dequantizes the output\n",
        "        self.quant2 = quant.QuantStub()  # Quantizes the input\n",
        "        self.dequant2 = quant.DeQuantStub()  # Dequantizes the output\n",
        "\n",
        "        #self.quant3 = quant.QuantStub()  # Quantizes the input\n",
        "        #self.dequant3 = quant.DeQuantStub()  # Dequantizes the output\n",
        "        #self.quant4 = quant.QuantStub()  # Quantizes the input\n",
        "        #self.dequant4 = quant.DeQuantStub()  # Dequantizes the output\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: [batch_size, seq_len, d_input]\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, d_input, seq_len]\n",
        "        #x = x.unsqueeze(1)  # Add a dummy dimension: [batch_size, 1, d_input, seq_len]\n",
        "        x = self.quant1(x)\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        #out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.dequant1(out)\n",
        "\n",
        "        out = self.quant2(out)\n",
        "        out = self.conv2(out)\n",
        "        #out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.dequant2(out)\n",
        "\n",
        "        #out = self.quant3(out)\n",
        "        #out = self.conv3(out)\n",
        "        #out = self.bn3(out)\n",
        "        #out = self.relu3(out)\n",
        "        #out = self.dequant3(out)\n",
        "        \n",
        "        #out = self.quant4(out)\n",
        "        #out = self.conv4(out)\n",
        "        #out = self.bn4(out)\n",
        "        #out = self.relu4(out)\n",
        "        #out = self.dequant4(out)\n",
        "\n",
        "        #out = out.squeeze(1)  # Remove the dummy dimension: [batch_size, d_output, seq_len]\n",
        "        out = out.permute(0, 2, 1)  # [batch_size, seq_len, d_output]\n",
        "        return out\n",
        "    \n",
        "class Encoder_q(nn.Module):\n",
        "    def __init__(self, d_input: int, d_model: int, d_output: int, seq_len: int):\n",
        "        super().__init__()\n",
        "        self.layer1 = QuantizedConv1d(in_channels=d_input, out_channels=d_model, kernel_size=3, padding=1)\n",
        "        self.layer2 = QuantizedConv1d(in_channels=d_model, out_channels=d_output, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        b, t, c = x.size()\n",
        "        out = self.layer1(x.permute(0, 2, 1))\n",
        "        self.layer1.apply_weight_fake_quant()  # Apply weight fake quantization for layer1\n",
        "        out = self.layer2(out)\n",
        "        self.layer2.apply_weight_fake_quant()  # Apply weight fake quantization for layer2\n",
        "        out = out.permute(0, 2, 1)  # (b, seq_len, d_output)\n",
        "        return out\n",
        "\n",
        "class QuantizedLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(QuantizedLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.ih = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
        "        self.hh = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
        "\n",
        "        self.quant1 = QuantStub()\n",
        "        self.dequant1 = DeQuantStub()\n",
        "\n",
        "        self.quant2 = QuantStub()\n",
        "        self.dequant2 = DeQuantStub()\n",
        "\n",
        "        self.quant3 = QuantStub()\n",
        "        self.dequant3 = DeQuantStub()\n",
        "\n",
        "\n",
        "    def forward(self, input, hx):\n",
        "        hx, cx = hx\n",
        "\n",
        "        # Quantize inputs\n",
        "        #input = self.quant1(input)\n",
        "        hx = self.quant2(hx)\n",
        "        cx = self.quant3(cx)\n",
        "\n",
        "        # LSTM cell operations\n",
        "        ih_out = self.ih(input)\n",
        "        hh_out = self.hh(hx)\n",
        "\n",
        "        # Dequantize before addition and multiplication\n",
        "        #ih_out = self.dequant1(ih_out)\n",
        "        hh_out = self.dequant2(hh_out)\n",
        "        cx = self.dequant3(cx)\n",
        "\n",
        "        gates = ih_out + hh_out\n",
        "\n",
        "        i, f, g, o = gates.chunk(4, 1)\n",
        "\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        g = torch.tanh(g)\n",
        "        o = torch.sigmoid(o)\n",
        "\n",
        "        cy = f * cx + i * g\n",
        "        hy = o * torch.tanh(cy)\n",
        "\n",
        "        return hy, cy\n",
        "\n",
        "    def _init_hidden(self, batch_size, device):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(batch_size, self.hidden_size).zero_().to(device),\n",
        "                weight.new(batch_size, self.hidden_size).zero_().to(device))\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_dim, decoder_dim, vocab, encoder_dim, device, dropout=0.5):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim, decoder_dim, bias=True)\n",
        "        #self.decode_step = QuantizedLSTMCell(embed_dim, decoder_dim, bias=True)\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "        #self.fc = nn.Linear(decoder_dim, self.vocab_size)\n",
        "        self.fc = nn.Linear(decoder_dim, self.vocab_size)\n",
        "        self.load_pretrained_embeddings()\n",
        "        \n",
        "        #EMBEDDING\n",
        "        #self.fake_quant = quant.FakeQuantize.with_args(observer=quant.default_observer, quant_min=-128, quant_max=127, dtype=torch.qint8)       \n",
        "        self.quantize_emb = QuantStub()\n",
        "        self.dequantize_emb = DeQuantStub()\n",
        "\n",
        "        #DECODER\n",
        "        self.quant_h = QuantStub()\n",
        "        self.dequant_h = DeQuantStub()\n",
        "        self.quant_c = QuantStub()\n",
        "        self.dequant_c = DeQuantStub()\n",
        "\n",
        "        #INIT \n",
        "        self.quant_init_h = QuantStub()\n",
        "        self.quant_init_c = QuantStub()\n",
        "        self.dequant_init_h = QuantStub()\n",
        "        self.dequant_init_c = DeQuantStub()\n",
        "        \n",
        "        #FC\n",
        "        self.quant_fc = QuantStub()\n",
        "        self.dequant_fc = DeQuantStub()\n",
        "\n",
        "    def load_pretrained_embeddings(self):\n",
        "        inputs = {\n",
        "            ModalityType.TEXT: semantichar.data.load_and_transform_text(self.vocab, self.device)\n",
        "        }\n",
        "        model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "        model.eval()\n",
        "        model.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(inputs)['text']\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "\n",
        "        #mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "\n",
        "        mean_encoder_out_h = self.quant_init_h(mean_encoder_out)\n",
        "        mean_encoder_out_c = self.quant_init_c(mean_encoder_out)\n",
        "\n",
        "        h = self.init_h(mean_encoder_out_h)\n",
        "        c = self.init_c(mean_encoder_out_c)\n",
        "\n",
        "        h = self.dequant_init_h(h)\n",
        "        c = self.dequant_init_c(c)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "        \n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
        "\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "        embeddings = self.embedding(encoded_captions.long())\n",
        "        #embeddings = self.fake_quant(embeddings)\n",
        "        q_emb = self.quantize_emb(embeddings)  # Quantize the embeddings\n",
        "        dq_emb = self.dequantize_emb(q_emb)  # Dequantize back to float\n",
        "\n",
        "        h, c = self.init_hidden_state(encoder_out)\n",
        "\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n",
        "\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            step_embeddings = dq_emb[:batch_size_t, t, :]\n",
        "            \n",
        "            h_batch = h[:batch_size_t]\n",
        "            c_batch = c[:batch_size_t]\n",
        "            #print( h_batch.shape)\n",
        "            #print( c_batch.shape)\n",
        "            h_quant = self.quant_h(h_batch)\n",
        "            c_quant = self.quant_c(c_batch)\n",
        "            \n",
        "            #print( h_quant.shape)\n",
        "            #print( c_quant.shape)\n",
        "\n",
        "            hx = ((h_quant, c_quant))           \n",
        "            #print(hx.shape) \n",
        "            h, c = self.decode_step(step_embeddings,hx)\n",
        "            \n",
        "            h_fc = self.quant_fc(h)\n",
        "            preds = self.fc(self.dropout(h_fc))\n",
        "            preds = self.dequant_fc(preds)\n",
        "\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, sort_ind\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.quantization as quant\n",
        "from torch.quantization.observer import MovingAverageMinMaxObserver, default_weight_observer\n",
        "\n",
        "#from semantichar.seq2seq import Encoder, Decoder\n",
        "#from semantichar.exp import trainer, evaluate\n",
        "from semantichar.dataset import prepare_dataset\n",
        "\n",
        "class CustomObserver(MovingAverageMinMaxObserver):\n",
        "    def calculate_qparams(self):\n",
        "        scale, _ = super().calculate_qparams()\n",
        "        zero_point = torch.tensor(0, dtype=torch.int32)\n",
        "        return scale, zero_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/keerthiv/HAR_models/SHARE/SHARE'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: easy_imu_phone\n",
            "data_path: ./\n",
            "manualSeed: 1\n",
            "epochs: 150\n",
            "early_stopping: 50\n",
            "batchSize: 16\n",
            "lr: 0.0001\n",
            "prob: 0.4\n",
            "cuda: True\n",
            "run_tag: test\n",
            "model_path: ./model/\n",
            "Random Seed:  1\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.quantization as quant\n",
        "#from custom_observer import CustomObserver  # Assuming you have a custom observer implementation\n",
        "from torch.quantization.observer import default_weight_observer\n",
        "import json\n",
        "\n",
        "# Updated values for the arguments\n",
        "dataset = 'easy_imu_phone'\n",
        "data_path = './'\n",
        "manualSeed = 1\n",
        "epochs = 150\n",
        "early_stopping = 50\n",
        "batchSize = 16\n",
        "lr = 1e-4\n",
        "prob = 0.4\n",
        "cuda = True\n",
        "run_tag = 'test'\n",
        "model_path = './model/'\n",
        "\n",
        "# Print the updated values\n",
        "print(f'dataset: {dataset}')\n",
        "print(f'data_path: {data_path}')\n",
        "print(f'manualSeed: {manualSeed}')\n",
        "print(f'epochs: {epochs}')\n",
        "print(f'early_stopping: {early_stopping}')\n",
        "print(f'batchSize: {batchSize}')\n",
        "print(f'lr: {lr}')\n",
        "print(f'prob: {prob}')\n",
        "print(f'cuda: {cuda}')\n",
        "print(f'run_tag: {run_tag}')\n",
        "print(f'model_path: {model_path}')\n",
        "\n",
        "# Set random seed\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "np.random.seed(manualSeed)\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not cuda:\n",
        "    print(\"You have a cuda device, so you might want to run with --cuda as option\")\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "\n",
        "data_root = data_path + '/dataset/' + dataset\n",
        "config_file = data_path + '/configs/' + dataset + '.json'\n",
        "with open(config_file, 'r') as config_file:\n",
        "    data = json.load(config_file)\n",
        "    label_dictionary = {int(k): v for k, v in data['label_dictionary'].items()}\n",
        "\n",
        "tr_data = np.load(data_root + '/x_train.npy')\n",
        "tr_label = np.load(data_root + '/y_train.npy')\n",
        "\n",
        "test_data = np.load(data_root + '/x_test.npy')\n",
        "test_label = np.load(data_root + '/y_test.npy')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available quantized engines: ['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# Custom qconfig enforcing per_tensor_affine\\ncustom_qconfig = quant.QConfig(\\n    activation=quant.default_observer.with_args(qscheme=torch.per_tensor_affine),\\n    weight=quant.default_weight_observer.with_args(qscheme=torch.per_tensor_affine)\\n)\\n\\n# Define custom QAT configuration\\ncustom_qconfig = torch.quantization.QConfig(\\n    activation=MovingAverageMinMaxObserver.with_args(dtype=torch.quint8, reduce_range=True),\\n    weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8)\\n)\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Available quantized engines:\", torch.backends.quantized.supported_engines)\n",
        "\n",
        "# Define quantization configurations\n",
        "#default_qconfig = quant.get_default_qconfig(torch.backends.quantized.engine)\n",
        "'''\n",
        "# Custom qconfig enforcing per_tensor_affine\n",
        "custom_qconfig = quant.QConfig(\n",
        "    activation=quant.default_observer.with_args(qscheme=torch.per_tensor_affine),\n",
        "    weight=quant.default_weight_observer.with_args(qscheme=torch.per_tensor_affine)\n",
        ")\n",
        "\n",
        "# Define custom QAT configuration\n",
        "custom_qconfig = torch.quantization.QConfig(\n",
        "    activation=MovingAverageMinMaxObserver.with_args(dtype=torch.quint8, reduce_range=True),\n",
        "    weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8)\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available quantized engines: ['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n",
            "load dataset\n",
            "torch.Size([4953, 150, 9]) torch.Size([1320, 150, 9])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/keerthiv/deep_conv_lstm/lib/python3.9/site-packages/semantichar/imagebind_model.py:515: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\".checkpoints/imagebind_huge.pth\"))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.quantization as quant\n",
        "from torch.quantization.observer import MovingAverageMinMaxObserver, PerChannelMinMaxObserver\n",
        "\n",
        "class CustomObserver(MovingAverageMinMaxObserver):\n",
        "    def calculate_qparams(self):\n",
        "        scale, _ = super().calculate_qparams()\n",
        "        zero_point = torch.tensor(0, dtype=torch.int32)\n",
        "        return scale, zero_point\n",
        "    \n",
        "# Check available quantized engines\n",
        "print(\"Available quantized engines:\", torch.backends.quantized.supported_engines)\n",
        "\n",
        "# Set the quantized engine\n",
        "torch.backends.quantized.engine = 'fbgemm'  # Use 'qnnpack' for ARM or mobile\n",
        "\n",
        "# Prepare dataset and models (assuming prepare_dataset is defined elsewhere)\n",
        "seq_len, dim, class_num, vocab_size, break_step, word_list, pred_dict, seqs, \\\n",
        "    tr_data, test_data, \\\n",
        "    tr_label, test_label, \\\n",
        "    tr_text, test_text, \\\n",
        "    len_text, test_len_text = prepare_dataset(tr_data, tr_label, test_data, test_label, label_dictionary)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "enc = Encoder(d_input=dim, d_model=128, d_output=128, seq_len=seq_len).to(device)\n",
        "dec = Decoder(embed_dim=1024, decoder_dim=128, vocab=word_list, encoder_dim=128, device=device).to(device)\n",
        "enc.eval()\n",
        "\n",
        "#dec.qconfig = custom_qconfig\n",
        "dec.qconfig = quant.get_default_qat_qconfig('fbgemm')\n",
        "enc.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "'''\n",
        "# Define a function to set qconfig for embedding layers\n",
        "def set_qconfig_for_embedding(module, qconfig):\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        module.qconfig = qconfig\n",
        "    for child in module.children():\n",
        "        set_qconfig_for_embedding(child, qconfig)\n",
        "'''\n",
        "# Apply the quantization configuration to the embedding layers in the decoder\n",
        "enc.train()\n",
        "dec.train()\n",
        "\n",
        "# Move the models to GPU for training\n",
        "enc.to(device)\n",
        "dec.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "params = list(enc.parameters()) + list(dec.parameters())\n",
        "optimizer = optim.Adam(params, lr=1e-4)\n",
        "cross_entropy = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration dictionary\n",
        "config = {\n",
        "    'batchSize': batchSize,\n",
        "    'epochs': epochs,\n",
        "    'run_tag': run_tag,\n",
        "    'dataset': dataset,\n",
        "    'cuda': cuda,\n",
        "    'manualSeed': manualSeed,\n",
        "    'data_path': data_path,\n",
        "    'early_stopping': early_stopping,\n",
        "    'lr': 0.0001,\n",
        "    'prob': prob,\n",
        "    'model_path': model_path\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10276/4159789132.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipykernel_10276/4159789132.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1 total loss: 1.7783\n",
            "epoch: 2 total loss: 1.1715\n",
            "epoch: 3 total loss: 1.0113\n",
            "epoch: 4 total loss: 0.9421\n",
            "epoch: 5 total loss: 0.8723\n",
            "epoch: 6 total loss: 0.8284\n",
            "epoch: 7 total loss: 0.7918\n",
            "epoch: 8 total loss: 0.7767\n",
            "epoch: 9 total loss: 0.7584\n",
            "epoch: 10 total loss: 0.7293\n",
            "epoch: 11 total loss: 0.7260\n",
            "epoch: 12 total loss: 0.7050\n",
            "epoch: 13 total loss: 0.6966\n",
            "epoch: 14 total loss: 0.6863\n",
            "epoch: 15 total loss: 0.6828\n",
            "epoch: 16 total loss: 0.6523\n",
            "epoch: 17 total loss: 0.6450\n",
            "epoch: 18 total loss: 0.6401\n",
            "epoch: 19 total loss: 0.6238\n",
            "epoch: 20 total loss: 0.6133\n",
            "epoch: 21 total loss: 0.5985\n",
            "epoch: 22 total loss: 0.5966\n",
            "epoch: 23 total loss: 0.6028\n",
            "epoch: 24 total loss: 0.5814\n",
            "epoch: 25 total loss: 0.5844\n",
            "epoch: 26 total loss: 0.5707\n",
            "epoch: 27 total loss: 0.5635\n",
            "epoch: 28 total loss: 0.5773\n",
            "epoch: 29 total loss: 0.5593\n",
            "epoch: 30 total loss: 0.5546\n",
            "epoch: 31 total loss: 0.5495\n",
            "epoch: 32 total loss: 0.5513\n",
            "epoch: 33 total loss: 0.5411\n",
            "epoch: 34 total loss: 0.5254\n",
            "epoch: 35 total loss: 0.5302\n",
            "epoch: 36 total loss: 0.5264\n",
            "epoch: 37 total loss: 0.5292\n",
            "epoch: 38 total loss: 0.5206\n",
            "epoch: 39 total loss: 0.5166\n",
            "epoch: 40 total loss: 0.5144\n",
            "epoch: 41 total loss: 0.5099\n",
            "epoch: 42 total loss: 0.5066\n",
            "epoch: 43 total loss: 0.5073\n",
            "epoch: 44 total loss: 0.5007\n",
            "epoch: 45 total loss: 0.4924\n",
            "epoch: 46 total loss: 0.4877\n",
            "epoch: 47 total loss: 0.4877\n",
            "epoch: 48 total loss: 0.4932\n",
            "epoch: 49 total loss: 0.4814\n",
            "epoch: 50 total loss: 0.4720\n",
            "epoch: 51 total loss: 0.4807\n",
            "epoch: 52 total loss: 0.4647\n",
            "epoch: 53 total loss: 0.4731\n",
            "epoch: 54 total loss: 0.4721\n",
            "epoch: 55 total loss: 0.4585\n",
            "epoch: 56 total loss: 0.4582\n",
            "epoch: 57 total loss: 0.4501\n",
            "epoch: 58 total loss: 0.4567\n",
            "epoch: 59 total loss: 0.4446\n",
            "epoch: 60 total loss: 0.4443\n",
            "epoch: 61 total loss: 0.4403\n",
            "epoch: 62 total loss: 0.4291\n",
            "epoch: 63 total loss: 0.4187\n",
            "epoch: 64 total loss: 0.4253\n",
            "epoch: 65 total loss: 0.4166\n",
            "epoch: 66 total loss: 0.4155\n",
            "epoch: 67 total loss: 0.4140\n",
            "epoch: 68 total loss: 0.4036\n",
            "epoch: 69 total loss: 0.3989\n",
            "epoch: 70 total loss: 0.3944\n",
            "epoch: 71 total loss: 0.3938\n",
            "epoch: 72 total loss: 0.3908\n",
            "epoch: 73 total loss: 0.3926\n",
            "epoch: 74 total loss: 0.3822\n",
            "epoch: 75 total loss: 0.3904\n",
            "epoch: 76 total loss: 0.3742\n",
            "epoch: 77 total loss: 0.3683\n",
            "epoch: 78 total loss: 0.3719\n",
            "epoch: 79 total loss: 0.3755\n",
            "epoch: 80 total loss: 0.3779\n",
            "epoch: 81 total loss: 0.3684\n",
            "epoch: 82 total loss: 0.3543\n",
            "epoch: 83 total loss: 0.3552\n",
            "epoch: 84 total loss: 0.3520\n",
            "epoch: 85 total loss: 0.3589\n",
            "epoch: 86 total loss: 0.3534\n",
            "epoch: 87 total loss: 0.3474\n",
            "epoch: 88 total loss: 0.3482\n",
            "epoch: 89 total loss: 0.3412\n",
            "epoch: 90 total loss: 0.3479\n",
            "epoch: 91 total loss: 0.3473\n",
            "epoch: 92 total loss: 0.3302\n",
            "epoch: 93 total loss: 0.3344\n",
            "epoch: 94 total loss: 0.3269\n",
            "epoch: 95 total loss: 0.3324\n",
            "epoch: 96 total loss: 0.3244\n",
            "epoch: 97 total loss: 0.3254\n",
            "epoch: 98 total loss: 0.3207\n",
            "epoch: 99 total loss: 0.3230\n",
            "epoch: 100 total loss: 0.3083\n",
            "epoch: 101 total loss: 0.3180\n",
            "epoch: 102 total loss: 0.3121\n",
            "epoch: 103 total loss: 0.3144\n",
            "epoch: 104 total loss: 0.3152\n",
            "epoch: 105 total loss: 0.3080\n",
            "epoch: 106 total loss: 0.3110\n",
            "epoch: 107 total loss: 0.3012\n",
            "epoch: 108 total loss: 0.2893\n",
            "epoch: 109 total loss: 0.3091\n",
            "epoch: 110 total loss: 0.2886\n",
            "epoch: 111 total loss: 0.3077\n",
            "epoch: 112 total loss: 0.2915\n",
            "epoch: 113 total loss: 0.2927\n",
            "epoch: 114 total loss: 0.2929\n",
            "epoch: 115 total loss: 0.2966\n",
            "epoch: 116 total loss: 0.2862\n",
            "epoch: 117 total loss: 0.2859\n",
            "epoch: 118 total loss: 0.2877\n",
            "epoch: 119 total loss: 0.2809\n",
            "epoch: 120 total loss: 0.2747\n",
            "epoch: 121 total loss: 0.2768\n",
            "epoch: 122 total loss: 0.2855\n",
            "epoch: 123 total loss: 0.2770\n",
            "epoch: 124 total loss: 0.2724\n",
            "epoch: 125 total loss: 0.2631\n",
            "epoch: 126 total loss: 0.2653\n",
            "epoch: 127 total loss: 0.2628\n",
            "epoch: 128 total loss: 0.2644\n",
            "epoch: 129 total loss: 0.2506\n",
            "epoch: 130 total loss: 0.2664\n",
            "epoch: 131 total loss: 0.2608\n",
            "epoch: 132 total loss: 0.2593\n",
            "epoch: 133 total loss: 0.2506\n",
            "epoch: 134 total loss: 0.2501\n",
            "epoch: 135 total loss: 0.2530\n",
            "epoch: 136 total loss: 0.2433\n",
            "epoch: 137 total loss: 0.2426\n",
            "epoch: 138 total loss: 0.2419\n",
            "epoch: 139 total loss: 0.2314\n",
            "epoch: 140 total loss: 0.2467\n",
            "epoch: 141 total loss: 0.2446\n",
            "epoch: 142 total loss: 0.2390\n",
            "epoch: 143 total loss: 0.2367\n",
            "epoch: 144 total loss: 0.2394\n",
            "epoch: 145 total loss: 0.2427\n",
            "epoch: 146 total loss: 0.2264\n",
            "epoch: 147 total loss: 0.2333\n",
            "epoch: 148 total loss: 0.2210\n",
            "epoch: 149 total loss: 0.2358\n",
            "epoch: 150 total loss: 0.2279\n",
            "epoch: 151 total loss: 0.2231\n",
            "epoch: 152 total loss: 0.2346\n",
            "epoch: 153 total loss: 0.2296\n",
            "epoch: 154 total loss: 0.2290\n",
            "epoch: 155 total loss: 0.2206\n",
            "epoch: 156 total loss: 0.2093\n",
            "epoch: 157 total loss: 0.2158\n",
            "epoch: 158 total loss: 0.2050\n",
            "epoch: 159 total loss: 0.2101\n",
            "epoch: 160 total loss: 0.2039\n",
            "epoch: 161 total loss: 0.2081\n",
            "epoch: 162 total loss: 0.2072\n",
            "epoch: 163 total loss: 0.2046\n",
            "epoch: 164 total loss: 0.2037\n",
            "epoch: 165 total loss: 0.2061\n",
            "epoch: 166 total loss: 0.2021\n",
            "epoch: 167 total loss: 0.1917\n",
            "epoch: 168 total loss: 0.2001\n",
            "epoch: 169 total loss: 0.2001\n",
            "epoch: 170 total loss: 0.1968\n",
            "epoch: 171 total loss: 0.2036\n",
            "epoch: 172 total loss: 0.1859\n",
            "epoch: 173 total loss: 0.1901\n",
            "epoch: 174 total loss: 0.1963\n",
            "epoch: 175 total loss: 0.1951\n",
            "epoch: 176 total loss: 0.1885\n",
            "epoch: 177 total loss: 0.1990\n",
            "epoch: 178 total loss: 0.1867\n",
            "epoch: 179 total loss: 0.1915\n",
            "epoch: 180 total loss: 0.1960\n",
            "epoch: 181 total loss: 0.1804\n",
            "epoch: 182 total loss: 0.1857\n",
            "epoch: 183 total loss: 0.1910\n",
            "epoch: 184 total loss: 0.1816\n",
            "epoch: 185 total loss: 0.1882\n",
            "epoch: 186 total loss: 0.1829\n",
            "epoch: 187 total loss: 0.1828\n",
            "epoch: 188 total loss: 0.1786\n",
            "epoch: 189 total loss: 0.1813\n",
            "epoch: 190 total loss: 0.1794\n",
            "epoch: 191 total loss: 0.1793\n",
            "epoch: 192 total loss: 0.1769\n",
            "epoch: 193 total loss: 0.1753\n",
            "epoch: 194 total loss: 0.1739\n",
            "epoch: 195 total loss: 0.1683\n",
            "epoch: 196 total loss: 0.1720\n",
            "epoch: 197 total loss: 0.1760\n",
            "epoch: 198 total loss: 0.1709\n",
            "epoch: 199 total loss: 0.1702\n",
            "epoch: 200 total loss: 0.1701\n",
            "epoch: 201 total loss: 0.1690\n",
            "epoch: 202 total loss: 0.1627\n",
            "epoch: 203 total loss: 0.1637\n",
            "epoch: 204 total loss: 0.1619\n",
            "epoch: 205 total loss: 0.1612\n",
            "epoch: 206 total loss: 0.1618\n",
            "epoch: 207 total loss: 0.1727\n",
            "epoch: 208 total loss: 0.1636\n",
            "epoch: 209 total loss: 0.1674\n",
            "epoch: 210 total loss: 0.1549\n",
            "epoch: 211 total loss: 0.1624\n",
            "epoch: 212 total loss: 0.1563\n",
            "epoch: 213 total loss: 0.1618\n",
            "epoch: 214 total loss: 0.1505\n",
            "epoch: 215 total loss: 0.1553\n",
            "epoch: 216 total loss: 0.1556\n",
            "epoch: 217 total loss: 0.1572\n",
            "epoch: 218 total loss: 0.1598\n",
            "epoch: 219 total loss: 0.1462\n",
            "epoch: 220 total loss: 0.1526\n",
            "epoch: 221 total loss: 0.1568\n",
            "epoch: 222 total loss: 0.1602\n",
            "epoch: 223 total loss: 0.1516\n",
            "epoch: 224 total loss: 0.1578\n",
            "epoch: 225 total loss: 0.1506\n",
            "epoch: 226 total loss: 0.1537\n",
            "epoch: 227 total loss: 0.1507\n",
            "epoch: 228 total loss: 0.1503\n",
            "epoch: 229 total loss: 0.1400\n",
            "epoch: 230 total loss: 0.1492\n",
            "epoch: 231 total loss: 0.1437\n",
            "epoch: 232 total loss: 0.1419\n",
            "epoch: 233 total loss: 0.1390\n",
            "epoch: 234 total loss: 0.1421\n",
            "epoch: 235 total loss: 0.1345\n",
            "epoch: 236 total loss: 0.1492\n",
            "epoch: 237 total loss: 0.1449\n",
            "epoch: 238 total loss: 0.1429\n",
            "epoch: 239 total loss: 0.1458\n",
            "epoch: 240 total loss: 0.1385\n",
            "epoch: 241 total loss: 0.1373\n",
            "epoch: 242 total loss: 0.1404\n",
            "epoch: 243 total loss: 0.1379\n",
            "epoch: 244 total loss: 0.1331\n",
            "epoch: 245 total loss: 0.1362\n",
            "epoch: 246 total loss: 0.1379\n",
            "epoch: 247 total loss: 0.1364\n",
            "epoch: 248 total loss: 0.1393\n",
            "epoch: 249 total loss: 0.1301\n",
            "epoch: 250 total loss: 0.1367\n",
            "epoch: 251 total loss: 0.1271\n",
            "epoch: 252 total loss: 0.1380\n",
            "epoch: 253 total loss: 0.1350\n",
            "epoch: 254 total loss: 0.1283\n",
            "epoch: 255 total loss: 0.1377\n",
            "epoch: 256 total loss: 0.1318\n",
            "epoch: 257 total loss: 0.1246\n",
            "epoch: 258 total loss: 0.1250\n",
            "epoch: 259 total loss: 0.1297\n",
            "epoch: 260 total loss: 0.1289\n",
            "epoch: 261 total loss: 0.1249\n",
            "epoch: 262 total loss: 0.1303\n",
            "epoch: 263 total loss: 0.1324\n",
            "epoch: 264 total loss: 0.1285\n",
            "epoch: 265 total loss: 0.1309\n",
            "epoch: 266 total loss: 0.1167\n",
            "epoch: 267 total loss: 0.1257\n",
            "epoch: 268 total loss: 0.1269\n",
            "epoch: 269 total loss: 0.1306\n",
            "epoch: 270 total loss: 0.1313\n",
            "epoch: 271 total loss: 0.1193\n",
            "epoch: 272 total loss: 0.1231\n",
            "epoch: 273 total loss: 0.1252\n",
            "epoch: 274 total loss: 0.1241\n",
            "epoch: 275 total loss: 0.1232\n",
            "epoch: 276 total loss: 0.1275\n",
            "epoch: 277 total loss: 0.1175\n",
            "epoch: 278 total loss: 0.1283\n",
            "epoch: 279 total loss: 0.1195\n",
            "epoch: 280 total loss: 0.1191\n",
            "epoch: 281 total loss: 0.1140\n",
            "epoch: 282 total loss: 0.1238\n",
            "epoch: 283 total loss: 0.1202\n",
            "epoch: 284 total loss: 0.1245\n",
            "epoch: 285 total loss: 0.1179\n",
            "epoch: 286 total loss: 0.1220\n",
            "epoch: 287 total loss: 0.1267\n",
            "epoch: 288 total loss: 0.1150\n",
            "epoch: 289 total loss: 0.1172\n",
            "epoch: 290 total loss: 0.1153\n",
            "epoch: 291 total loss: 0.1202\n",
            "epoch: 292 total loss: 0.1144\n",
            "epoch: 293 total loss: 0.1195\n",
            "epoch: 294 total loss: 0.1151\n",
            "epoch: 295 total loss: 0.1190\n",
            "epoch: 296 total loss: 0.1175\n",
            "epoch: 297 total loss: 0.1148\n",
            "epoch: 298 total loss: 0.1137\n",
            "epoch: 299 total loss: 0.1158\n",
            "epoch: 300 total loss: 0.1219\n",
            "epoch: 301 total loss: 0.1150\n",
            "epoch: 302 total loss: 0.1175\n",
            "epoch: 303 total loss: 0.1167\n",
            "epoch: 304 total loss: 0.1148\n",
            "epoch: 305 total loss: 0.1140\n",
            "epoch: 306 total loss: 0.1189\n",
            "epoch: 307 total loss: 0.1135\n",
            "epoch: 308 total loss: 0.1088\n",
            "epoch: 309 total loss: 0.1145\n",
            "epoch: 310 total loss: 0.1078\n",
            "epoch: 311 total loss: 0.1081\n",
            "epoch: 312 total loss: 0.1140\n",
            "epoch: 313 total loss: 0.1078\n",
            "epoch: 314 total loss: 0.1126\n",
            "epoch: 315 total loss: 0.1166\n",
            "epoch: 316 total loss: 0.1133\n",
            "epoch: 317 total loss: 0.1098\n",
            "epoch: 318 total loss: 0.1015\n",
            "epoch: 319 total loss: 0.1105\n",
            "epoch: 320 total loss: 0.1131\n",
            "epoch: 321 total loss: 0.1147\n",
            "epoch: 322 total loss: 0.1053\n",
            "epoch: 323 total loss: 0.1075\n",
            "epoch: 324 total loss: 0.1145\n",
            "epoch: 325 total loss: 0.1064\n",
            "epoch: 326 total loss: 0.1072\n",
            "epoch: 327 total loss: 0.1031\n",
            "epoch: 328 total loss: 0.1087\n",
            "epoch: 329 total loss: 0.1155\n",
            "epoch: 330 total loss: 0.1075\n",
            "epoch: 331 total loss: 0.1107\n",
            "epoch: 332 total loss: 0.1129\n",
            "epoch: 333 total loss: 0.1044\n",
            "epoch: 334 total loss: 0.1085\n",
            "epoch: 335 total loss: 0.1146\n",
            "epoch: 336 total loss: 0.1111\n",
            "epoch: 337 total loss: 0.1101\n",
            "epoch: 338 total loss: 0.1041\n",
            "epoch: 339 total loss: 0.1089\n",
            "epoch: 340 total loss: 0.1035\n",
            "epoch: 341 total loss: 0.0976\n",
            "epoch: 342 total loss: 0.1104\n",
            "epoch: 343 total loss: 0.1117\n",
            "epoch: 344 total loss: 0.1063\n",
            "epoch: 345 total loss: 0.1036\n",
            "epoch: 346 total loss: 0.1015\n",
            "epoch: 347 total loss: 0.1019\n",
            "epoch: 348 total loss: 0.1011\n",
            "epoch: 349 total loss: 0.1073\n",
            "epoch: 350 total loss: 0.1069\n",
            "epoch: 351 total loss: 0.1034\n",
            "epoch: 352 total loss: 0.1108\n",
            "epoch: 353 total loss: 0.1073\n",
            "epoch: 354 total loss: 0.1161\n",
            "epoch: 355 total loss: 0.1012\n",
            "epoch: 356 total loss: 0.1071\n",
            "epoch: 357 total loss: 0.1034\n",
            "epoch: 358 total loss: 0.1140\n",
            "epoch: 359 total loss: 0.1064\n",
            "epoch: 360 total loss: 0.1110\n",
            "epoch: 361 total loss: 0.1050\n",
            "epoch: 362 total loss: 0.1048\n",
            "epoch: 363 total loss: 0.0953\n",
            "epoch: 364 total loss: 0.1047\n",
            "epoch: 365 total loss: 0.1009\n",
            "epoch: 366 total loss: 0.1011\n",
            "epoch: 367 total loss: 0.1040\n",
            "epoch: 368 total loss: 0.1013\n",
            "epoch: 369 total loss: 0.1026\n",
            "epoch: 370 total loss: 0.1056\n",
            "epoch: 371 total loss: 0.1047\n",
            "epoch: 372 total loss: 0.1067\n",
            "epoch: 373 total loss: 0.1074\n",
            "epoch: 374 total loss: 0.1038\n",
            "epoch: 375 total loss: 0.1060\n",
            "epoch: 376 total loss: 0.1101\n",
            "epoch: 377 total loss: 0.1022\n",
            "epoch: 378 total loss: 0.1020\n",
            "epoch: 379 total loss: 0.1045\n",
            "epoch: 380 total loss: 0.0975\n",
            "epoch: 381 total loss: 0.1013\n",
            "epoch: 382 total loss: 0.0987\n",
            "epoch: 383 total loss: 0.1019\n",
            "epoch: 384 total loss: 0.1079\n",
            "epoch: 385 total loss: 0.1039\n",
            "epoch: 386 total loss: 0.0977\n",
            "epoch: 387 total loss: 0.0979\n",
            "epoch: 388 total loss: 0.0946\n",
            "epoch: 389 total loss: 0.1036\n",
            "epoch: 390 total loss: 0.1028\n",
            "epoch: 391 total loss: 0.0953\n",
            "epoch: 392 total loss: 0.0994\n",
            "epoch: 393 total loss: 0.0995\n",
            "epoch: 394 total loss: 0.0906\n",
            "epoch: 395 total loss: 0.1035\n",
            "epoch: 396 total loss: 0.0932\n",
            "epoch: 397 total loss: 0.1065\n",
            "epoch: 398 total loss: 0.1044\n",
            "epoch: 399 total loss: 0.0972\n",
            "epoch: 400 total loss: 0.0840\n",
            "epoch: 401 total loss: 0.0897\n",
            "epoch: 402 total loss: 0.0995\n",
            "epoch: 403 total loss: 0.0966\n",
            "epoch: 404 total loss: 0.0968\n",
            "epoch: 405 total loss: 0.0979\n",
            "epoch: 406 total loss: 0.1022\n",
            "epoch: 407 total loss: 0.0984\n",
            "epoch: 408 total loss: 0.0984\n",
            "epoch: 409 total loss: 0.0948\n",
            "epoch: 410 total loss: 0.0968\n",
            "epoch: 411 total loss: 0.1005\n",
            "epoch: 412 total loss: 0.0933\n",
            "epoch: 413 total loss: 0.1034\n",
            "epoch: 414 total loss: 0.1026\n",
            "epoch: 415 total loss: 0.0951\n",
            "epoch: 416 total loss: 0.0909\n",
            "epoch: 417 total loss: 0.0946\n",
            "epoch: 418 total loss: 0.0972\n",
            "epoch: 419 total loss: 0.0946\n",
            "epoch: 420 total loss: 0.0939\n",
            "epoch: 421 total loss: 0.0999\n",
            "epoch: 422 total loss: 0.1042\n",
            "epoch: 423 total loss: 0.0920\n",
            "epoch: 424 total loss: 0.0913\n",
            "epoch: 425 total loss: 0.0944\n",
            "epoch: 426 total loss: 0.0954\n",
            "epoch: 427 total loss: 0.0915\n",
            "epoch: 428 total loss: 0.0913\n",
            "epoch: 429 total loss: 0.0917\n",
            "epoch: 430 total loss: 0.0979\n",
            "epoch: 431 total loss: 0.0961\n",
            "epoch: 432 total loss: 0.0945\n",
            "epoch: 433 total loss: 0.1002\n",
            "epoch: 434 total loss: 0.0954\n",
            "epoch: 435 total loss: 0.0890\n",
            "epoch: 436 total loss: 0.0936\n",
            "epoch: 437 total loss: 0.1041\n",
            "epoch: 438 total loss: 0.0928\n",
            "epoch: 439 total loss: 0.0944\n",
            "epoch: 440 total loss: 0.0851\n",
            "epoch: 441 total loss: 0.0997\n",
            "epoch: 442 total loss: 0.0922\n",
            "epoch: 443 total loss: 0.0908\n",
            "epoch: 444 total loss: 0.0880\n",
            "epoch: 445 total loss: 0.1025\n",
            "epoch: 446 total loss: 0.0957\n",
            "epoch: 447 total loss: 0.0868\n",
            "epoch: 448 total loss: 0.0931\n",
            "epoch: 449 total loss: 0.0939\n",
            "epoch: 450 total loss: 0.0881\n",
            "epoch: 451 total loss: 0.0974\n",
            "epoch: 452 total loss: 0.0969\n",
            "epoch: 453 total loss: 0.0873\n",
            "epoch: 454 total loss: 0.0922\n",
            "epoch: 455 total loss: 0.0891\n",
            "epoch: 456 total loss: 0.0986\n",
            "epoch: 457 total loss: 0.0935\n",
            "epoch: 458 total loss: 0.0854\n",
            "epoch: 459 total loss: 0.0990\n",
            "epoch: 460 total loss: 0.0928\n",
            "epoch: 461 total loss: 0.0919\n",
            "epoch: 462 total loss: 0.0898\n",
            "epoch: 463 total loss: 0.0838\n",
            "epoch: 464 total loss: 0.0921\n",
            "epoch: 465 total loss: 0.0874\n",
            "epoch: 466 total loss: 0.1159\n",
            "epoch: 467 total loss: 0.0933\n",
            "epoch: 468 total loss: 0.0941\n",
            "epoch: 469 total loss: 0.0872\n",
            "epoch: 470 total loss: 0.0893\n",
            "epoch: 471 total loss: 0.0950\n",
            "epoch: 472 total loss: 0.0980\n",
            "epoch: 473 total loss: 0.0840\n",
            "epoch: 474 total loss: 0.0827\n",
            "epoch: 475 total loss: 0.0897\n",
            "epoch: 476 total loss: 0.0893\n",
            "epoch: 477 total loss: 0.0954\n",
            "epoch: 478 total loss: 0.0979\n",
            "epoch: 479 total loss: 0.0907\n",
            "epoch: 480 total loss: 0.0832\n",
            "epoch: 481 total loss: 0.0918\n",
            "epoch: 482 total loss: 0.0915\n",
            "epoch: 483 total loss: 0.0862\n",
            "epoch: 484 total loss: 0.0874\n",
            "epoch: 485 total loss: 0.0875\n",
            "epoch: 486 total loss: 0.0903\n",
            "epoch: 487 total loss: 0.0870\n",
            "epoch: 488 total loss: 0.0959\n",
            "epoch: 489 total loss: 0.0967\n",
            "epoch: 490 total loss: 0.0965\n",
            "epoch: 491 total loss: 0.0875\n",
            "epoch: 492 total loss: 0.0883\n",
            "epoch: 493 total loss: 0.0946\n",
            "epoch: 494 total loss: 0.0909\n",
            "epoch: 495 total loss: 0.0905\n",
            "epoch: 496 total loss: 0.0875\n",
            "epoch: 497 total loss: 0.0830\n",
            "epoch: 498 total loss: 0.0890\n",
            "epoch: 499 total loss: 0.0822\n",
            "epoch: 500 total loss: 0.0879\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "enc.to(device)\n",
        "dec.to(device)\n",
        "tr_data.to(device)\n",
        "tr_label.to(device)\n",
        "tr_text.to(device)\n",
        "len_text.to(device)\n",
        "\n",
        "for epoch in range(500):\n",
        "    loss = trainer(\n",
        "        config, # configs\n",
        "        enc, # encoder\n",
        "        dec, # decoder\n",
        "        cross_entropy, # loss\n",
        "        optimizer, # optimizer\n",
        "        tr_data, # training input\n",
        "        tr_label, # training labels\n",
        "        tr_text, # training label text sequence\n",
        "        len_text, # training label text sequence length\n",
        "        break_step, # max training label text sequence length\n",
        "        vocab_size, # vocabulary size\n",
        "        device, # device\n",
        "    )\n",
        "\n",
        "    print(\"epoch: %d total loss: %.4f\" % (epoch + 1, loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (conv1): Conv1d(9, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu1): ReLU()\n",
              "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu2): ReLU()\n",
              "  (quant1): QuantStub()\n",
              "  (dequant1): DeQuantStub()\n",
              "  (quant2): QuantStub()\n",
              "  (dequant2): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "import torch\n",
        "\n",
        "def fold_batch_norm(conv, bn):\n",
        "    with torch.no_grad():\n",
        "        # Ensure the batch norm parameters are on the same device as the convolution weights\n",
        "        if conv.weight.device != bn.weight.device:\n",
        "            bn = bn.to(conv.weight.device)\n",
        "        \n",
        "        # Calculate scale and shift factors\n",
        "        scale_factor = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
        "        shift_factor = bn.bias - bn.running_mean * scale_factor\n",
        "\n",
        "        # Reshape scale_factor to match convolution weight dimensions for 1D convolution\n",
        "        scale_factor = scale_factor.reshape([-1, 1, 1])\n",
        "        \n",
        "        # Apply scaling to convolution weights\n",
        "        conv.weight.copy_(conv.weight * scale_factor)\n",
        "        \n",
        "        # Handle convolution bias\n",
        "        if conv.bias is None:\n",
        "            conv.bias = torch.nn.Parameter(shift_factor)\n",
        "        else:\n",
        "            conv.bias.copy_((conv.bias - bn.running_mean) * scale_factor.squeeze() + bn.bias)\n",
        "    \n",
        "    return conv\n",
        "\n",
        "# Example usage:\n",
        "enc.to('cpu')  # Ensure the model is on CPU for parameter updates\n",
        "enc.conv1 = fold_batch_norm(enc.conv1, enc.bn1)\n",
        "enc.conv2 = fold_batch_norm(enc.conv2, enc.bn2)\n",
        "\n",
        "# Remove batch norm layers after folding\n",
        "del enc.bn1\n",
        "del enc.bn2\n",
        "\n",
        "# Define the new forward method without BatchNorm layers\n",
        "def new_forward(self, x):\n",
        "    x = self.quant1(x.permute(0, 2, 1))  # Quantize the input\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.dequant1(x)\n",
        "    x = self.quant2(x)  # Quantize the input\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.dequant2(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    return x\n",
        "\n",
        "# Attach the new forward method to the model instance\n",
        "enc.forward = new_forward.__get__(enc, Encoder)\n",
        "\n",
        "# Verify the modified forward method\n",
        "print(enc.forward)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder(\n",
            "  (conv1): Conv1d(9, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU()\n",
            "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU()\n",
            "  (quant1): QuantStub()\n",
            "  (dequant1): DeQuantStub()\n",
            "  (quant2): QuantStub()\n",
            "  (dequant2): DeQuantStub()\n",
            ")\n",
            "Encoder(\n",
            "  (conv1): Conv1d(9, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU()\n",
            "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU()\n",
            "  (quant1): QuantStub()\n",
            "  (dequant1): DeQuantStub()\n",
            "  (quant2): QuantStub()\n",
            "  (dequant2): DeQuantStub()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "enc.eval()\n",
        "#enc = torch.quantization.fuse_modules(enc, [['conv1', 'bn1', 'relu1'], ['conv2', 'bn2', 'relu2'], ['conv3', 'bn3', 'relu3'],  ['conv4', 'bn4', 'relu4']])\n",
        "enc.to('cpu')\n",
        "enc.eval()\n",
        "print( enc )\n",
        "#quantized_enc = Encoder(9,128,128,150)\n",
        "#quantized_enc = copy.deepcopy(enc)\n",
        "#quantized_enc.conv1 = enc.conv1\n",
        "#quantized_enc.conv2 = quant.convert(enc.conv2, inplace=False)\n",
        "quantized_enc = quant.convert(enc, inplace=False)\n",
        "print( quantized_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder(\n",
            "  (embedding): Embedding(37, 1024)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (decode_step): LSTMCell(1024, 128)\n",
            "  (init_h): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (init_c): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=128, out_features=37, bias=True)\n",
            "  (quantize_emb): QuantStub()\n",
            "  (dequantize_emb): DeQuantStub()\n",
            "  (quant_h): QuantStub()\n",
            "  (dequant_h): DeQuantStub()\n",
            "  (quant_c): QuantStub()\n",
            "  (dequant_c): DeQuantStub()\n",
            "  (quant_init_h): QuantStub()\n",
            "  (quant_init_c): QuantStub()\n",
            "  (dequant_init_h): QuantStub()\n",
            "  (dequant_init_c): DeQuantStub()\n",
            "  (quant_fc): QuantStub()\n",
            "  (dequant_fc): DeQuantStub()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "custom_qconfig = quant.QConfig(\n",
        "    activation=quant.default_observer,\n",
        "    weight=quant.default_weight_observer\n",
        ")\n",
        "float_qparams_weight_only_qconfig = quant.float_qparams_weight_only_qconfig\n",
        "# Apply the quantization configuration to the embedding layers again before conversion\n",
        "#set_qconfig_for_embedding(dec, float_qparams_weight_only_qconfig)\n",
        "#enc.eval()\n",
        "dec.eval()\n",
        "#enc.layer1.fuse_model()\n",
        "#enc.layer2.fuse_model()\n",
        "#enc.qconfig = default_qconfig\n",
        "#enc.cpu()\n",
        "dec.cpu()\n",
        "\n",
        "\n",
        "# Convert models to quantized versions\n",
        "#quantized_enc = quant.convert(enc, inplace=False)\n",
        "#print(\"Encoder model converted to quantized version\")\n",
        "\n",
        "quantized_dec = quant.convert(dec, inplace=False)\n",
        "#print(\"Decoder model converted to quantized version\")\n",
        "\n",
        "# Print the quantized models\n",
        "#print(quantized_enc)\n",
        "print(quantized_dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the quantized models\n",
        "torch.save(enc.state_dict(), model_path + run_tag + '_enc.pth')\n",
        "torch.save(dec.state_dict(), model_path + run_tag + '_dec.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the quantized models\n",
        "torch.save(quantized_enc.state_dict(), model_path + run_tag + 'quant_enc.pth')\n",
        "torch.save(quantized_dec.state_dict(), model_path + run_tag + 'quant_dec.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1 Evaluation Time: 0.04 seconds\n",
            "Batch 2 Evaluation Time: 0.03 seconds\n",
            "Batch 3 Evaluation Time: 0.03 seconds\n",
            "Batch 4 Evaluation Time: 0.03 seconds\n",
            "Batch 5 Evaluation Time: 0.04 seconds\n",
            "Batch 6 Evaluation Time: 0.04 seconds\n",
            "Batch 7 Evaluation Time: 0.04 seconds\n",
            "Batch 8 Evaluation Time: 0.03 seconds\n",
            "Batch 9 Evaluation Time: 0.04 seconds\n",
            "Batch 10 Evaluation Time: 0.03 seconds\n",
            "Batch 11 Evaluation Time: 0.03 seconds\n",
            "Batch 12 Evaluation Time: 0.03 seconds\n",
            "Batch 13 Evaluation Time: 0.03 seconds\n",
            "Batch 14 Evaluation Time: 0.03 seconds\n",
            "Batch 15 Evaluation Time: 0.04 seconds\n",
            "Batch 16 Evaluation Time: 0.03 seconds\n",
            "Batch 17 Evaluation Time: 0.04 seconds\n",
            "Batch 18 Evaluation Time: 0.04 seconds\n",
            "Batch 19 Evaluation Time: 0.04 seconds\n",
            "Batch 20 Evaluation Time: 0.04 seconds\n",
            "Batch 21 Evaluation Time: 0.04 seconds\n",
            "Batch 22 Evaluation Time: 0.04 seconds\n",
            "Batch 23 Evaluation Time: 0.04 seconds\n",
            "Batch 24 Evaluation Time: 0.04 seconds\n",
            "Batch 25 Evaluation Time: 0.04 seconds\n",
            "Batch 26 Evaluation Time: 0.04 seconds\n",
            "Batch 27 Evaluation Time: 0.03 seconds\n",
            "Batch 28 Evaluation Time: 0.03 seconds\n",
            "Batch 29 Evaluation Time: 0.03 seconds\n",
            "Batch 30 Evaluation Time: 0.04 seconds\n",
            "Batch 31 Evaluation Time: 0.03 seconds\n",
            "Batch 32 Evaluation Time: 0.04 seconds\n",
            "Batch 33 Evaluation Time: 0.03 seconds\n",
            "Batch 34 Evaluation Time: 0.03 seconds\n",
            "Batch 35 Evaluation Time: 0.03 seconds\n",
            "Batch 36 Evaluation Time: 0.03 seconds\n",
            "Batch 37 Evaluation Time: 0.03 seconds\n",
            "Batch 38 Evaluation Time: 0.04 seconds\n",
            "Batch 39 Evaluation Time: 0.04 seconds\n",
            "Batch 40 Evaluation Time: 0.03 seconds\n",
            "Batch 41 Evaluation Time: 0.03 seconds\n",
            "Batch 42 Evaluation Time: 0.03 seconds\n",
            "Batch 43 Evaluation Time: 0.03 seconds\n",
            "Batch 44 Evaluation Time: 0.03 seconds\n",
            "Batch 45 Evaluation Time: 0.03 seconds\n",
            "Batch 46 Evaluation Time: 0.03 seconds\n",
            "Batch 47 Evaluation Time: 0.03 seconds\n",
            "Batch 48 Evaluation Time: 0.04 seconds\n",
            "Batch 49 Evaluation Time: 0.03 seconds\n",
            "Batch 50 Evaluation Time: 0.03 seconds\n",
            "Batch 51 Evaluation Time: 0.03 seconds\n",
            "Batch 52 Evaluation Time: 0.03 seconds\n",
            "Batch 53 Evaluation Time: 0.03 seconds\n",
            "Batch 54 Evaluation Time: 0.03 seconds\n",
            "Batch 55 Evaluation Time: 0.03 seconds\n",
            "Batch 56 Evaluation Time: 0.04 seconds\n",
            "Batch 57 Evaluation Time: 0.03 seconds\n",
            "Batch 58 Evaluation Time: 0.03 seconds\n",
            "Batch 59 Evaluation Time: 0.03 seconds\n",
            "Batch 60 Evaluation Time: 0.03 seconds\n",
            "Batch 61 Evaluation Time: 0.03 seconds\n",
            "Batch 62 Evaluation Time: 0.03 seconds\n",
            "Batch 63 Evaluation Time: 0.03 seconds\n",
            "Batch 64 Evaluation Time: 0.03 seconds\n",
            "Batch 65 Evaluation Time: 0.03 seconds\n",
            "Batch 66 Evaluation Time: 0.03 seconds\n",
            "Batch 67 Evaluation Time: 0.03 seconds\n",
            "Batch 68 Evaluation Time: 0.04 seconds\n",
            "Batch 69 Evaluation Time: 0.04 seconds\n",
            "Batch 70 Evaluation Time: 0.03 seconds\n",
            "Batch 71 Evaluation Time: 0.03 seconds\n",
            "Batch 72 Evaluation Time: 0.03 seconds\n",
            "Batch 73 Evaluation Time: 0.03 seconds\n",
            "Batch 74 Evaluation Time: 0.03 seconds\n",
            "Batch 75 Evaluation Time: 0.03 seconds\n",
            "Batch 76 Evaluation Time: 0.03 seconds\n",
            "Batch 77 Evaluation Time: 0.03 seconds\n",
            "Batch 78 Evaluation Time: 0.03 seconds\n",
            "Batch 79 Evaluation Time: 0.03 seconds\n",
            "Batch 80 Evaluation Time: 0.03 seconds\n",
            "Batch 81 Evaluation Time: 0.03 seconds\n",
            "Batch 82 Evaluation Time: 0.03 seconds\n",
            "Batch 83 Evaluation Time: 0.02 seconds\n",
            "Total Evaluation Time: 2.86 seconds\n",
            "Average Evaluation Time per Batch: 0.03 seconds\n",
            "Average Evaluation Time per Sample: 0.002164 seconds\n",
            "Test Acc: 0.9152 Macro-Prec: 0.9177 Macro-Rec: 0.9152 Macro-F1: 0.9139\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "dec.eval()\n",
        "dec.cpu()\n",
        "enc.eval()\n",
        "enc.cpu()\n",
        "\n",
        "hypotheses = list()\n",
        "batch_size = test_data.size(0)\n",
        "pred_whole = torch.zeros_like(test_label)\n",
        "seqs = seqs.to(device)\n",
        "\n",
        "total_evaluation_time = 0  # Initialize total evaluation time\n",
        "total_samples = 0  # Initialize total number of samples\n",
        "\n",
        "for batch_idx, (batch_data, batch_label, batch_text, batch_len) in enumerate(\n",
        "        DataBatch(test_data, test_label, test_text, test_len_text, config['batchSize'], shuffle=False)\n",
        "    ):\n",
        "\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_label = batch_label.to(device)\n",
        "        batch_text = batch_text.to(device)\n",
        "        batch_len = batch_len.to(device)\n",
        "        \n",
        "        # Start timing after sending to device\n",
        "        start_time = time.time()\n",
        "\n",
        "        batch_size = batch_data.size(0)\n",
        "        total_samples += batch_size  # Accumulate the number of samples\n",
        "        encoder_out = quantized_enc(batch_data)  # (batch_size, enc_seq_len, encoder_dim)\n",
        "        enc_seq_len = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(2)\n",
        "\n",
        "        encoder_out = encoder_out.unsqueeze(1).expand(batch_size, class_num, enc_seq_len, encoder_dim)\n",
        "        encoder_out = encoder_out.reshape(batch_size * class_num, enc_seq_len, encoder_dim)\n",
        "\n",
        "        k_prev_words = seqs[:, 0].unsqueeze(0).expand(batch_size, class_num).long()  # (batch_size, class_num)\n",
        "        k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
        "\n",
        "        h, c = quantized_dec.init_hidden_state(encoder_out)\n",
        "\n",
        "        seq_scores = torch.zeros((batch_size, class_num)).to(device)\n",
        "\n",
        "        for step in range(1, break_step):\n",
        "            embeddings = quantized_dec.embedding(k_prev_words).squeeze(1)  # (batch_size * class_num, embed_dim)\n",
        "            h, c = quantized_dec.decode_step(embeddings, (h, c))\n",
        "            scores = quantized_dec.fc(h.reshape(batch_size, class_num, -1))  # (batch_size, class_num, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=-1)\n",
        "            k_prev_words = seqs[:, step].unsqueeze(0).expand(batch_size, class_num).long()\n",
        "            for batch_i in range(batch_size):\n",
        "                for class_i in range(class_num):\n",
        "                    if k_prev_words[batch_i, class_i] != 0:\n",
        "                        seq_scores[batch_i, class_i] += scores[batch_i, class_i, k_prev_words[batch_i, class_i]]\n",
        "            k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
        "\n",
        "        max_indices = seq_scores.argmax(dim=1)\n",
        "        for batch_i in range(batch_size):\n",
        "            max_i = max_indices[batch_i]\n",
        "            seq = seqs[max_i].tolist()\n",
        "            hypotheses.append([w for w in seq if w not in {0, vocab_size - 1}])\n",
        "            pred_whole[batch_i + batch_idx * config['batchSize']] = pred_dict[\"#\".join(map(str, hypotheses[-1]))]\n",
        "\n",
        "        # End timing for the batch\n",
        "        end_time = time.time()\n",
        "        batch_evaluation_time = end_time - start_time  # Calculate batch evaluation time\n",
        "        total_evaluation_time += batch_evaluation_time  # Accumulate total evaluation time\n",
        "\n",
        "        print(f'Batch {batch_idx + 1} Evaluation Time: {batch_evaluation_time:.2f} seconds')\n",
        "\n",
        "acc = accuracy_score(test_label.cpu().numpy(), pred_whole.cpu().numpy())\n",
        "prec = precision_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "rec = recall_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "f1 = f1_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "\n",
        "print(f'Total Evaluation Time: {total_evaluation_time:.2f} seconds')\n",
        "    \n",
        "    # Calculate evaluation time per batch and per sample\n",
        "eval_time_per_batch = total_evaluation_time / (batch_idx + 1)\n",
        "eval_time_per_sample = total_evaluation_time / total_samples\n",
        "\n",
        "print(f'Average Evaluation Time per Batch: {eval_time_per_batch:.2f} seconds')\n",
        "print(f'Average Evaluation Time per Sample: {eval_time_per_sample:.6f} seconds')\n",
        "print('Test Acc: %.4f Macro-Prec: %.4f Macro-Rec: %.4f Macro-F1: %.4f' % (acc, prec, rec, f1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/keerthiv/deep_conv_lstm/lib/python3.9/site-packages/semantichar/imagebind_model.py:515: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\".checkpoints/imagebind_huge.pth\"))\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "tensor(0.)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m dec_loaded \u001b[38;5;241m=\u001b[39m Decoder(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, decoder_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, vocab\u001b[38;5;241m=\u001b[39mword_list, encoder_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare dataset and models (assuming prepare_dataset is defined elsewhere)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m seq_len, dim, class_num, vocab_size, break_step, word_list, pred_dict, seqs, \\\n\u001b[1;32m      9\u001b[0m     tr_data, test_data, \\\n\u001b[1;32m     10\u001b[0m     tr_label, test_label, \\\n\u001b[1;32m     11\u001b[0m     tr_text, test_text, \\\n\u001b[0;32m---> 12\u001b[0m     len_text, test_len_text \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_dictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Prepare the recreated models for QAT\u001b[39;00m\n\u001b[1;32m     16\u001b[0m enc_loaded\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;241m=\u001b[39m quant\u001b[38;5;241m.\u001b[39mget_default_qat_qconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqnnpack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/deep_conv_lstm/lib/python3.9/site-packages/semantichar/dataset.py:49\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(tr_data, tr_label, test_data, test_label, label_dictionary)\u001b[0m\n\u001b[1;32m     46\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_list)\n\u001b[1;32m     48\u001b[0m onehot_dict \u001b[38;5;241m=\u001b[39m {k:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mv\u001b[38;5;241m+\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m label_dictionary\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 49\u001b[0m tr_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([onehot(l, break_step, onehot_dict, word_list)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m tr_label], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     50\u001b[0m len_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([onehot(l, break_step, onehot_dict, word_list)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m tr_label])\n\u001b[1;32m     51\u001b[0m test_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([onehot(l, break_step, onehot_dict, word_list)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m test_label], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m~/deep_conv_lstm/lib/python3.9/site-packages/semantichar/dataset.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_list)\n\u001b[1;32m     48\u001b[0m onehot_dict \u001b[38;5;241m=\u001b[39m {k:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mv\u001b[38;5;241m+\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m label_dictionary\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 49\u001b[0m tr_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\u001b[43monehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreak_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monehot_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_list\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m tr_label], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     50\u001b[0m len_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([onehot(l, break_step, onehot_dict, word_list)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m tr_label])\n\u001b[1;32m     51\u001b[0m test_text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([onehot(l, break_step, onehot_dict, word_list)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m test_label], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m~/deep_conv_lstm/lib/python3.9/site-packages/semantichar/dataset.py:6\u001b[0m, in \u001b[0;36monehot\u001b[0;34m(l, k, onehot_dict, word_list)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21monehot\u001b[39m(l, k, onehot_dict, word_list):\n\u001b[1;32m      5\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(k)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43monehot_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m):\n\u001b[1;32m      7\u001b[0m         encoding[i] \u001b[38;5;241m=\u001b[39m word_list\u001b[38;5;241m.\u001b[39mindex(label)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding, \u001b[38;5;28mlen\u001b[39m(onehot_dict[l])\n",
            "\u001b[0;31mKeyError\u001b[0m: tensor(0.)"
          ]
        }
      ],
      "source": [
        "#to be run on raspberry pi \n",
        "torch.backends.quantized.engine = 'qnnpack'\n",
        "\n",
        "enc_loaded = Encoder(d_input=dim, d_model=128, d_output=128, seq_len=seq_len).to(device)\n",
        "dec_loaded = Decoder(embed_dim=1024, decoder_dim=128, vocab=word_list, encoder_dim=128, device=device).to(device)\n",
        "\n",
        "# Prepare dataset and models (assuming prepare_dataset is defined elsewhere)\n",
        "seq_len, dim, class_num, vocab_size, break_step, word_list, pred_dict, seqs, \\\n",
        "    tr_data, test_data, \\\n",
        "    tr_label, test_label, \\\n",
        "    tr_text, test_text, \\\n",
        "    len_text, test_len_text = prepare_dataset(tr_data, tr_label, test_data, test_label, label_dictionary)\n",
        "\n",
        "\n",
        "# Prepare the recreated models for QAT\n",
        "enc_loaded.qconfig = quant.get_default_qat_qconfig('qnnpack')\n",
        "dec_loaded.qconfig = quant.get_default_qat_qconfig('qnnpack')\n",
        "quant.prepare_qat(enc_loaded, inplace=True)\n",
        "quant.prepare_qat(dec_loaded, inplace=True)\n",
        "quant.convert(enc_loaded, inplace=True)\n",
        "quant.convert(dec_loaded, inplace=True)\n",
        "\n",
        "# Load the saved state_dicts into the recreated models\n",
        "enc_loaded.load_state_dict(torch.load(model_path + run_tag + 'quant_enc.pth'))\n",
        "dec_loaded.load_state_dict(torch.load(model_path + run_tag + 'quant_dec.pth'))\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "enc_loaded.eval()\n",
        "enc_loaded.cpu()\n",
        "dec_loaded.eval()\n",
        "dec_loaded.cpu()\n",
        "\n",
        "hypotheses = list()\n",
        "batch_size = test_data.size(0)\n",
        "pred_whole = torch.zeros_like(test_label)\n",
        "seqs = seqs.to(device)\n",
        "\n",
        "total_evaluation_time = 0  # Initialize total evaluation time\n",
        "total_samples = 0  # Initialize total number of samples\n",
        "\n",
        "for batch_idx, (batch_data, batch_label, batch_text, batch_len) in enumerate(\n",
        "        DataBatch(test_data, test_label, test_text, test_len_text, config['batchSize'], shuffle=False)\n",
        "    ):\n",
        "\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_label = batch_label.to(device)\n",
        "        batch_text = batch_text.to(device)\n",
        "        batch_len = batch_len.to(device)\n",
        "        \n",
        "        # Start timing after sending to device\n",
        "        start_time = time.time()\n",
        "\n",
        "        batch_size = batch_data.size(0)\n",
        "        total_samples += batch_size  # Accumulate the number of samples\n",
        "        encoder_out = enc_loaded(batch_data)  # (batch_size, enc_seq_len, encoder_dim)\n",
        "        enc_seq_len = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(2)\n",
        "\n",
        "        encoder_out = encoder_out.unsqueeze(1).expand(batch_size, class_num, enc_seq_len, encoder_dim)\n",
        "        encoder_out = encoder_out.reshape(batch_size * class_num, enc_seq_len, encoder_dim)\n",
        "\n",
        "        k_prev_words = seqs[:, 0].unsqueeze(0).expand(batch_size, class_num).long()  # (batch_size, class_num)\n",
        "        k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
        "\n",
        "        h, c = dec_loaded.init_hidden_state(encoder_out)\n",
        "\n",
        "        seq_scores = torch.zeros((batch_size, class_num)).to(device)\n",
        "\n",
        "        for step in range(1, break_step):\n",
        "            embeddings = dec_loaded.embedding(k_prev_words).squeeze(1)  # (batch_size * class_num, embed_dim)\n",
        "            h, c = dec_loaded.decode_step(embeddings, (h, c))\n",
        "            scores = dec_loaded.fc(h.reshape(batch_size, class_num, -1))  # (batch_size, class_num, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=-1)\n",
        "            k_prev_words = seqs[:, step].unsqueeze(0).expand(batch_size, class_num).long()\n",
        "            for batch_i in range(batch_size):\n",
        "                for class_i in range(class_num):\n",
        "                    if k_prev_words[batch_i, class_i] != 0:\n",
        "                        seq_scores[batch_i, class_i] += scores[batch_i, class_i, k_prev_words[batch_i, class_i]]\n",
        "            k_prev_words = k_prev_words.reshape(batch_size * class_num, 1)  # (batch_size * class_num, 1)\n",
        "\n",
        "        max_indices = seq_scores.argmax(dim=1)\n",
        "        for batch_i in range(batch_size):\n",
        "            max_i = max_indices[batch_i]\n",
        "            seq = seqs[max_i].tolist()\n",
        "            hypotheses.append([w for w in seq if w not in {0, vocab_size - 1}])\n",
        "            pred_whole[batch_i + batch_idx * config['batchSize']] = pred_dict[\"#\".join(map(str, hypotheses[-1]))]\n",
        "\n",
        "        # End timing for the batch\n",
        "        end_time = time.time()\n",
        "        batch_evaluation_time = end_time - start_time  # Calculate batch evaluation time\n",
        "        total_evaluation_time += batch_evaluation_time  # Accumulate total evaluation time\n",
        "\n",
        "        print(f'Batch {batch_idx + 1} Evaluation Time: {batch_evaluation_time:.2f} seconds')\n",
        "\n",
        "acc = accuracy_score(test_label.cpu().numpy(), pred_whole.cpu().numpy())\n",
        "prec = precision_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "rec = recall_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "f1 = f1_score(test_label.cpu().numpy(), pred_whole.cpu().numpy(), average='macro', zero_division=0)\n",
        "\n",
        "print(f'Total Evaluation Time: {total_evaluation_time:.2f} seconds')\n",
        "    \n",
        "    # Calculate evaluation time per batch and per sample\n",
        "eval_time_per_batch = total_evaluation_time / (batch_idx + 1)\n",
        "eval_time_per_sample = total_evaluation_time / total_samples\n",
        "\n",
        "print(f'Average Evaluation Time per Batch: {eval_time_per_batch:.2f} seconds')\n",
        "print(f'Average Evaluation Time per Sample: {eval_time_per_sample:.6f} seconds')\n",
        "print('Test Acc: %.4f Macro-Prec: %.4f Macro-Rec: %.4f Macro-F1: %.4f' % (acc, prec, rec, f1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
